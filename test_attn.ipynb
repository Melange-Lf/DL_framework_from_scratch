{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e94082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model import Model\n",
    "import func\n",
    "from layers import Linear, ReLU, Conv2D, Flatten, LazyLinear, Attention\n",
    "# import torch\n",
    "# from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94574cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot_vector(labels, low, high):\n",
    "    labels = np.asarray(labels).flatten()\n",
    "    num_classes = high - low + 1\n",
    "\n",
    "    if np.any((labels < low) | (labels > high)):\n",
    "        raise ValueError(\"Some labels are outside the specified (low, high) range.\")\n",
    "\n",
    "    one_hot = np.eye(num_classes)[labels - low]\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5c4fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(16, 24, 24)\n",
      "(16, 24, 24)\n",
      "(16, 20, 20)\n",
      "(16, 20, 20)\n",
      "(16, 20, 20)\n",
      "(32, 18, 18)\n",
      "(32, 18, 18)\n",
      "(32, 18, 18)\n",
      "(64, 16, 16)\n",
      "(64, 16, 16)\n",
      "(64, 16, 16)\n",
      "(64, 14, 14)\n",
      "(64, 14, 14)\n",
      "12544\n",
      "128\n",
      "128\n",
      "64\n",
      "64\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# model = Model([Linear(28*28, 64), Sigmoid(64, 64), Linear(64, 32), Sigmoid(32, 32), Linear(32, 16), Sigmoid(16, 16), Linear(16, 10)])\n",
    "# model = Model([Linear(28*28, 64), ReLU(64, 64), Linear(64, 32), ReLU(32, 32), Linear(32, 16), ReLU(16, 16), Linear(16, 10)])\n",
    "model = Model([\n",
    "    Conv2D(5, 16, \"Valid\", (1, 28, 28)),\n",
    "    ReLU(),\n",
    "    Conv2D(5, 16, \"Valid\"),\n",
    "    ReLU(),\n",
    "    Attention(32), # not introducing another activation function as softmax is already used inside, plus want to better prevserve the attnetion results\n",
    "    Conv2D(3, 32, \"Valid\"),\n",
    "    ReLU(),\n",
    "    Attention(64),\n",
    "    Conv2D(3, 64, \"valid\"),\n",
    "    ReLU(),\n",
    "    Attention(128), \n",
    "    Conv2D(3, 64, \"valid\"),\n",
    "    ReLU(),\n",
    "    Flatten(),\n",
    "    LazyLinear(128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 10)\n",
    "], show_dims=True)\n",
    "acc = lambda log, lab: np.mean(np.argmax(log, axis=1) == np.argmax(lab, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b69c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(0.001, func.cross_entropy_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96bd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_fname = \"MNIST_CSV/mnist_train.csv\"\n",
    "ts_fname = \"MNIST_CSV/mnist_test.csv\"\n",
    "\n",
    "train_data = np.loadtxt(tr_fname, delimiter=',')\n",
    "train_labels = to_one_hot_vector(train_data[:, 0].astype(int), low=0, high=9)\n",
    "train_images = train_data[:, 1:].reshape(-1, 1, 28, 28)\n",
    "\n",
    "test_data = np.loadtxt(ts_fname, delimiter=',')\n",
    "test_labels = to_one_hot_vector(test_data[:, 0].astype(int), 0, 9)\n",
    "test_images = test_data[:, 1:].reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f28cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch_loss: 15.779229232595988, batch_metric: 0.125, batch: 1/938, epoch: 0\n",
      "\n",
      "batch_loss: 7.026195326173831, batch_metric: 0.09375, batch: 94/938, epoch: 0\n",
      "\n",
      "batch_loss: 4.2055048903633185, batch_metric: 0.140625, batch: 187/938, epoch: 0\n",
      "\n",
      "batch_loss: 3.2507586216952906, batch_metric: 0.203125, batch: 280/938, epoch: 0\n",
      "\n",
      "batch_loss: 2.747001803339646, batch_metric: 0.234375, batch: 373/938, epoch: 0\n",
      "\n",
      "batch_loss: 2.721189493041144, batch_metric: 0.3125, batch: 466/938, epoch: 0\n",
      "\n",
      "batch_loss: 1.9728424446551291, batch_metric: 0.3125, batch: 559/938, epoch: 0\n",
      "\n",
      "batch_loss: 2.2107317090518235, batch_metric: 0.328125, batch: 652/938, epoch: 0\n",
      "\n",
      "batch_loss: 2.0275123941560587, batch_metric: 0.390625, batch: 745/938, epoch: 0\n",
      "\n",
      "batch_loss: 2.0204486861753272, batch_metric: 0.375, batch: 838/938, epoch: 0\n",
      "\n",
      "batch_loss: 2.1614509045261032, batch_metric: 0.390625, batch: 931/938, epoch: 0\n",
      "\n",
      "\n",
      "========================epoch_loss: 3.484542987886998, epoch_metric: 0.2486507196162047, epoch: 0\n",
      "\n",
      "\n",
      "\n",
      "batch_loss: 2.0937244805389157, batch_metric: 0.34375, batch: 1/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.6769373834846557, batch_metric: 0.5, batch: 94/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.8717862504156002, batch_metric: 0.328125, batch: 187/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.672681633996043, batch_metric: 0.4375, batch: 280/938, epoch: 1\n",
      "\n",
      "batch_loss: 2.0489424040219966, batch_metric: 0.375, batch: 373/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.8009938080226124, batch_metric: 0.375, batch: 466/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.753458785741096, batch_metric: 0.4375, batch: 559/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.9011976987027273, batch_metric: 0.421875, batch: 652/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.3440992068527995, batch_metric: 0.53125, batch: 745/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.5757734463747437, batch_metric: 0.546875, batch: 838/938, epoch: 1\n",
      "\n",
      "batch_loss: 1.8201095204671023, batch_metric: 0.390625, batch: 931/938, epoch: 1\n",
      "\n",
      "\n",
      "========================epoch_loss: 1.7502984570581654, epoch_metric: 0.43969882729211085, epoch: 1\n",
      "\n",
      "\n",
      "\n",
      "batch_loss: 1.3398818505739176, batch_metric: 0.515625, batch: 1/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.6845879242515776, batch_metric: 0.5, batch: 94/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.7761109242670752, batch_metric: 0.5, batch: 187/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.4870239618089947, batch_metric: 0.5, batch: 280/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.2356171019286761, batch_metric: 0.5625, batch: 373/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.27803387913498, batch_metric: 0.59375, batch: 466/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.3306921391328528, batch_metric: 0.609375, batch: 559/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.5113929547097174, batch_metric: 0.5625, batch: 652/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.1910103325440462, batch_metric: 0.5625, batch: 745/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.0263584240781922, batch_metric: 0.65625, batch: 838/938, epoch: 2\n",
      "\n",
      "batch_loss: 1.1691636237567007, batch_metric: 0.6875, batch: 931/938, epoch: 2\n",
      "\n",
      "\n",
      "========================epoch_loss: 1.369431874130934, epoch_metric: 0.5704790778251599, epoch: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size=64, epochs= 3, print_period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c6fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
